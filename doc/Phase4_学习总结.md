# Phase 4ï¼šä»æ•°æ®åˆ°è®­ç»ƒä¸æ¨ç†çš„å®Œæ•´å®æˆ˜

> ğŸ¯ ç›®æ ‡ï¼šå¸¦ä½ ä»é›¶è·‘é€šä¸€æ¬¡ UIE è®­ç»ƒ + æ¨ç†ï¼Œå…¨ç¨‹å¯¹ç…§é¡¹ç›®æºä»£ç ï¼Œåšåˆ°â€œçŸ¥é“æ¯ä¸€è¡Œåœ¨å¹²ä»€ä¹ˆâ€ã€‚

æœ¬ç« æŒ‰çœŸå®å¼€å‘æµç¨‹æ¥èµ°ï¼š

1. å…ˆæŠŠåŸå§‹ CMeIE æ•°æ®è½¬æ¢æˆ UIE æ ¼å¼ï¼ˆçœ‹æ‡‚ `convert_data.py`ï¼‰
2. å†ç”¨ä¸€é”®è„šæœ¬è·‘è®­ç»ƒï¼ˆé¡ºç€çœ‹ `run_training_debug.sh` å’Œ `finetune.py`ï¼‰
3. æœ€åç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åšä¸€æ¬¡æ¨ç†ï¼ˆé˜…è¯» `uie_predictor.py` çš„æ ¸å¿ƒé€»è¾‘ï¼‰

å»ºè®®è¾¹çœ‹è¾¹åœ¨é¡¹ç›®æ ¹ç›®å½•å¼€ä¸€ä¸ªç»ˆç«¯ï¼ŒæŠŠä»£ç å’Œæ—¥å¿—ä¸€èµ·å¯¹ç…§ç€çœ‹ã€‚

---

## 0. å‰ç½®å‡†å¤‡

åœ¨é¡¹ç›®æ ¹ç›®å½• `ai-medical/` ä¸‹ï¼Œå»ºè®®å…ˆåˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼Œç„¶åå®‰è£…ä¾èµ–ï¼š

```bash
cd /Users/deyong/PycharmProjects/medical/ai-medical
pip install -r uie_pytorch/requirements.txt
```

ç¡®è®¤ `debug_data/` ç›®å½•ä¸‹å·²æœ‰ `train.jsonl`ã€`dev.jsonl` ç­‰ CMeIE æ ¼å¼æ•°æ®ã€‚

---

## 1. æ€»ä½“è·¯çº¿å›¾ï¼šä»æ–‡ä»¶åˆ°æ¨¡å‹

å…ˆå¼„æ¸…æ¥šæ•´ä½“æ•°æ®å’Œä»£ç æ˜¯å¦‚ä½•ä¸²èµ·æ¥çš„ï¼š

```mermaid
graph LR
    A["debug_data/train.jsonl"] --> B["convert_data.py"]
    B --> C["debug_data/train_converted.jsonl"]
    C --> D["run_training_debug.sh"]
    D --> E["uie_pytorch/finetune.py do_train"]
    E --> F["checkpoint_debug/model_best"]
    F --> G["uie_pytorch/uie_predictor.py UIEPredictor"]
```

åé¢æ¯ä¸€å°èŠ‚ï¼Œéƒ½ä¼šæ˜ç¡®ï¼š
- â€œç°åœ¨ä½ åº”è¯¥åšä»€ä¹ˆå‘½ä»¤â€
- â€œè¿™æ¡å‘½ä»¤ä¼šè°ƒç”¨å“ªä¸ªè„šæœ¬â€
- â€œè„šæœ¬é‡Œçš„æ¯ä¸€æ®µä»£ç åœ¨å¹²ä»€ä¹ˆâ€

---

## 2. ç¬¬ä¸€æ­¥ï¼šæŠŠ CMeIE æ•°æ®è½¬æ¢æˆ UIE æ ·æœ¬

### 2.1 å…ˆæ‰§è¡Œå‘½ä»¤ï¼Œå†çœ‹ä»£ç 

åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼ˆå¦‚æœå·²ç»è·‘è¿‡å¯ä»¥è·³è¿‡ï¼Œä½†å»ºè®®å†è·‘ä¸€éä½“ä¼šæµç¨‹ï¼‰ï¼š

```bash
# è½¬æ¢è®­ç»ƒé›†
python3 convert_data.py \
  --input_file debug_data/train.jsonl \
  --output_file debug_data/train_converted.jsonl

# è½¬æ¢éªŒè¯é›†
python3 convert_data.py \
  --input_file debug_data/dev.jsonl \
  --output_file debug_data/dev_converted.jsonl
```

å®Œæˆåï¼Œå¯ä»¥ç”¨ `head` çœ‹ä¸€ä¸‹è¾“å‡ºæ ¼å¼ï¼š

```bash
head -n 3 debug_data/train_converted.jsonl
```

ä¸€è¡Œå°±æ˜¯ä¸€æ¡ UIE è®­ç»ƒæ ·æœ¬ï¼Œå…¸å‹ç»“æ„ï¼š

```json
{
  "content": "åŸå§‹å¥å­æ–‡æœ¬",
  "prompt": "ç–¾ç—…",
  "result_list": [
    {"text": "ç³–å°¿ç—…", "start": 10, "end": 13}
  ]
}
```

### 2.2 å¯¹ç…§æºç é˜…è¯» `convert_data.py`

æ–‡ä»¶ï¼š`convert_data.py`

#### (1) æ€»å…¥å£ï¼š`if __name__ == "__main__":`

```python
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True)
    parser.add_argument("--output_file", type=str, required=True)
    args = parser.parse_args()
    
    convert_file(args.input_file, args.output_file)
```

é€è¡Œç†è§£ï¼š
- `ArgumentParser()`ï¼šå£°æ˜è¿™æ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œè„šæœ¬
- `--input_file / --output_file`ï¼šå¯¹åº”ä½ åˆšåˆšåœ¨å‘½ä»¤è¡Œä¼ å…¥çš„å‚æ•°
- `parse_args()`ï¼šè§£æå‘½ä»¤è¡Œ
- `convert_file(...)`ï¼šç”¨è§£æå‡ºçš„è·¯å¾„çœŸæ­£å»åšè½¬æ¢

#### (2) å•æ–‡ä»¶è½¬æ¢æµç¨‹ï¼š`convert_file`

```python
def convert_file(input_path, output_path):
    print(f"Converting {input_path} to {output_path}...")
    with open(input_path, 'r', encoding='utf-8') as f_in, \
         open(output_path, 'w', encoding='utf-8') as f_out:
        
        for line in tqdm(f_in):
            try:
                cmeie_data = json.loads(line)
                uie_samples = cmeie_to_uie_samples(cmeie_data)
                for sample in uie_samples:
                    f_out.write(json.dumps(sample, ensure_ascii=False) + '\n')
            except Exception as e:
                print(f"Error processing line: {e}")
```

å…³é”®ç‚¹ï¼š
- `for line in tqdm(f_in)`ï¼šä¸€è¡Œä¸€è¡Œè¯»åŸå§‹ CMeIE æ•°æ®ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦æ¡
- `json.loads(line)`ï¼šæŠŠå­—ç¬¦ä¸²è§£ææˆ Python `dict`
- `cmeie_to_uie_samples(...)`ï¼šæ ¸å¿ƒè½¬æ¢é€»è¾‘ï¼Œä¸€æ¡ CMeIE å¯èƒ½ä¼šå˜æˆå¤šæ¡ UIE æ ·æœ¬
- å†™æ–‡ä»¶æ—¶æ¯ä¸ª `sample` å†è½¬å› JSON å­—ç¬¦ä¸²ï¼Œä¸€è¡Œä¸€æ¡

#### (3) ç»“æ„è½¬æ¢æ ¸å¿ƒï¼š`cmeie_to_uie_samples`

```python
def cmeie_to_uie_samples(cmeie_data):
    text = cmeie_data['text']
    spo_list = cmeie_data.get('spo_list', [])
    
    uie_samples = []
    
    # ç”¨äºå»é‡çš„é›†åˆ
    seen_entities = set()
    seen_relations = set()
    
    for spo in spo_list:
        subject = spo['subject']
        subject_type = spo['subject_type']
        predicate = spo['predicate']
        obj = spo['object']['@value']
        obj_type = spo['object_type']['@value']
        ...
```

ä½ éœ€è¦ç†è§£çš„å‡ ä¸ªå…³é”®å˜é‡ï¼š
- `text`ï¼šä¸€æ¡æ ·æœ¬çš„åŸå§‹å¥å­
- `spo_list`ï¼šCMeIE ä¸­çš„ä¸‰å…ƒç»„åˆ—è¡¨ï¼ˆsubjectâ€“predicateâ€“objectï¼‰
- `seen_entities` / `seen_relations`ï¼šé˜²æ­¢é‡å¤ç”ŸæˆåŒæ ·çš„æ ·æœ¬

å‡½æ•°å†…éƒ¨åˆ†ä¸‰ç±»æ ·æœ¬æ„é€ ï¼š

1. **Subject å®ä½“æŠ½å–æ ·æœ¬**
   ```python
   subject_key = (subject_type, subject)
   if subject_key not in seen_entities:
       seen_entities.add(subject_key)
       start = text.find(subject)
       if start != -1:
           uie_sample = {
               "content": text,
               "prompt": subject_type,
               "result_list": [{
                   "text": subject,
                   "start": start,
                   "end": start + len(subject)
               }]
           }
           uie_samples.append(uie_sample)
   ```
   - Promptï¼šå®ä½“ç±»å‹ï¼ˆæ¯”å¦‚â€œç–¾ç—…â€ï¼‰
   - Resultï¼šå¥å­ä¸­è¯¥å®ä½“å…·ä½“å‡ºç°çš„ä½ç½®

2. **Object å®ä½“æŠ½å–æ ·æœ¬**
   å’Œ Subject ç±»ä¼¼ï¼Œåªæ˜¯ç”¨çš„æ˜¯ object åŠå…¶ç±»å‹ã€‚

3. **å…³ç³»æŠ½å–æ ·æœ¬**
   ```python
   relation_key = (subject, predicate, obj)
   if relation_key not in seen_relations:
       seen_relations.add(relation_key)
       start = text.find(obj)
       if start != -1:
           uie_sample = {
               "content": text,
               "prompt": f"{subject}çš„{predicate}",
               "result_list": [{
                   "text": obj,
                   "start": start,
                   "end": start + len(obj)
               }]
           }
           uie_samples.append(uie_sample)
   ```

ç†è§£åˆ°è¿™é‡Œï¼Œä½ å°±æ¸…æ¥šï¼š
- CMeIE çš„ä¸€æ¡æ ·æœ¬ä¼šæ‹†æˆå¤šæ¡ UIE æ ·æœ¬
- UIE è®­ç»ƒåªä¾èµ–ä¸‰ä»¶äº‹ï¼šåŸå§‹å¥å­ã€Promptã€ç›®æ ‡ span

---

## 3. ç¬¬äºŒæ­¥ï¼šä¸€é”®è„šæœ¬è·‘è®­ç»ƒ + è¯»æ‡‚è®­ç»ƒä¸»å¾ªç¯

### 3.1 å…ˆè·‘èµ·æ¥ï¼š`run_training_debug.sh`

æ–‡ä»¶ï¼š`run_training_debug.sh`

å…ˆèµ‹äºˆæ‰§è¡Œæƒé™å¹¶è¿è¡Œï¼š

```bash
chmod +x run_training_debug.sh
./run_training_debug.sh
```

ä½ ä¼šçœ‹åˆ°ç±»ä¼¼ï¼š

```text
ğŸš€ å¼€å§‹ UIE æ¨¡å‹å®æˆ˜è®­ç»ƒ (Debug æ¨¡å¼)...
Training Epoch 1:  10%|...
global step 2, epoch: 1, loss: ...
Evaluation precision: ..., recall: ..., F1: ...
âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./checkpoint_debug
```

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¼šä¿å­˜åˆ°ï¼š
- `checkpoint_debug/model_best/`ï¼šæœ€ä½³ F1 çš„æ¨¡å‹ï¼ˆæ¨èæ¨ç†ç”¨ï¼‰

### 3.2 å¯¹ç…§è„šæœ¬é€è¡Œç†è§£

```bash
#!/bin/bash

set -e
echo "ğŸš€ å¼€å§‹ UIE æ¨¡å‹å®æˆ˜è®­ç»ƒ (Debug æ¨¡å¼)..."

export CUDA_VISIBLE_DEVICES=0

python3 uie_pytorch/finetune.py \
    --train_path "debug_data/train_converted.jsonl" \
    --dev_path "debug_data/dev_converted.jsonl" \
    --save_dir "./checkpoint_debug" \
    --learning_rate 1e-4 \
    --batch_size 4 \
    --max_seq_len 512 \
    --num_epochs 10 \
    --model "bert-base-chinese" \
    --logging_steps 2 \
    --valid_steps 5 \
    --device "cpu"
```

è¡Œçº§è¯´æ˜ï¼š
- `set -e`ï¼šä»»ä½•ä¸€è¡Œå‘½ä»¤å‡ºé”™ï¼Œæ•´ä¸ªè„šæœ¬ç«‹å³é€€å‡ºï¼Œé¿å…è·‘é”™ä¸€åŠ
- `export CUDA_VISIBLE_DEVICES=0`ï¼šå¯é€‰ï¼ŒæŒ‡å®šä½¿ç”¨å“ªå— GPUï¼ˆè¿™é‡Œè®­ç»ƒå®é™…ç”¨äº† `--device "cpu"`ï¼‰
- `python3 uie_pytorch/finetune.py \ ...`ï¼šçœŸæ­£å¯åŠ¨è®­ç»ƒ
  - `--train_path / --dev_path`ï¼šåˆšæ‰è½¬æ¢å¥½çš„ UIE æ ¼å¼æ•°æ®
  - `--save_dir`ï¼šæ¨¡å‹ä¿å­˜ç›®å½•
  - `--learning_rate`ï¼šå­¦ä¹ ç‡
  - `--batch_size`ï¼šæ¯ä¸ª batch çš„æ ·æœ¬æ•°
  - `--num_epochs`ï¼šè®­ç»ƒè½®æ•°
  - `--logging_steps`ï¼šæ¯å¤šå°‘ step æ‰“ä¸€æ¬¡è®­ç»ƒæ—¥å¿—
  - `--valid_steps`ï¼šæ¯å¤šå°‘ step è·‘ä¸€æ¬¡éªŒè¯é›†
  - `--device`ï¼š`cpu` æˆ– `gpu`

å»ºè®®ï¼šä½ å¯ä»¥æ”¹å‡ ä¸ªå‚æ•°ï¼ˆä¾‹å¦‚ `num_epochs=2`ï¼‰é‡æ–°è·‘ä¸€éï¼Œè§‚å¯Ÿæ—¥å¿—å˜åŒ–ã€‚

---

## 4. æ·±å…¥è®­ç»ƒæ ¸å¿ƒï¼šè¯»æ‡‚ `finetune.py::do_train`

æ–‡ä»¶ï¼š`uie_pytorch/finetune.py`

### 4.1 å…¥å£å‚æ•°è§£æ

åº•éƒ¨æ˜¯æ ‡å‡†çš„å‘½ä»¤è¡Œå…¥å£ï¼š

```python
if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("-b", "--batch_size", default=16, type=int)
    parser.add_argument("--learning_rate", default=1e-5, type=float)
    parser.add_argument("-t", "--train_path", default=None, required=True, type=str)
    parser.add_argument("-d", "--dev_path", default=None, required=True, type=str)
    parser.add_argument("-s", "--save_dir", default='./checkpoint', type=str)
    parser.add_argument("--max_seq_len", default=512, type=int)
    parser.add_argument("--num_epochs", default=100, type=int)
    parser.add_argument("--seed", default=1000, type=int)
    parser.add_argument("--logging_steps", default=10, type=int)
    parser.add_argument("--valid_steps", default=100, type=int)
    parser.add_argument("-D", '--device', choices=['cpu', 'gpu'], default="gpu")
    parser.add_argument("-m", "--model", default="uie_base_pytorch", type=str)
    parser.add_argument("--max_model_num", default=5, type=int)
    parser.add_argument("--early_stopping", action='store_true', default=False)

    args = parser.parse_args()
    do_train()
```

ç†è§£è¦ç‚¹ï¼š
- è¿™é‡Œçš„å‚æ•°å€¼å°±æ¥è‡ªä½ åœ¨ `run_training_debug.sh` ä¸­ä¼ çš„é‚£äº› `--xxx`
- å…¨å±€å˜é‡ `args` ä¼šåœ¨ `do_train()` é‡Œé¢è¢«ä½¿ç”¨

### 4.2 è®­ç»ƒå‡†å¤‡é˜¶æ®µ

```python
def do_train():
    set_seed(args.seed)
    show_bar = True

    tokenizer = BertTokenizerFast.from_pretrained(args.model)
    model = UIE.from_pretrained(args.model)
    if args.device == 'gpu':
        model = model.cuda()
```

é€å¥è¯´æ˜ï¼š
- `set_seed(args.seed)`ï¼šå›ºå®šéšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡è¿è¡Œç»“æœå°½é‡å¯å¤ç°
- `show_bar = True`ï¼šæ˜¯å¦æ˜¾ç¤º tqdm è¿›åº¦æ¡
- `BertTokenizerFast.from_pretrained(args.model)`ï¼š
  - ä» HuggingFace æˆ–æœ¬åœ°ç›®å½•åŠ è½½åˆ†è¯å™¨
  - è¿™é‡Œ `args.model="bert-base-chinese"`ï¼Œè¡¨ç¤ºç”¨ BERT ä¸­æ–‡ base æ¨¡å‹
- `UIE.from_pretrained(args.model)`ï¼š
  - ä»¥åŒåæ¨¡å‹åˆå§‹åŒ– UIE
  - å†…éƒ¨ä¼šåŠ è½½å¯¹åº”æƒé‡æ–‡ä»¶
- `model.cuda()`ï¼šå¦‚æœé€‰æ‹© `gpu`ï¼ŒæŠŠæ¨¡å‹ç§»åŠ¨åˆ°æ˜¾å­˜ä¸Š

æ¥ä¸‹æ¥æ˜¯æ•°æ®é›†å’Œ DataLoaderï¼š

```python
    train_ds = IEDataset(args.train_path, tokenizer=tokenizer,
                         max_seq_len=args.max_seq_len)
    dev_ds = IEDataset(args.dev_path, tokenizer=tokenizer,
                       max_seq_len=args.max_seq_len)

    train_data_loader = DataLoader(
        train_ds, batch_size=args.batch_size, shuffle=True)
    dev_data_loader = DataLoader(
        dev_ds, batch_size=args.batch_size, shuffle=True)
```

å«ä¹‰ï¼š
- `IEDataset`ï¼šè‡ªå®šä¹‰æ•°æ®é›†ç±»
  - è¯»å– `train_converted.jsonl`
  - è°ƒç”¨ tokenizer æŠŠæ–‡æœ¬è½¬æˆ `input_ids`/`token_type_ids`/`attention_mask`
  - åŒæ—¶ç”Ÿæˆå¯¹åº”çš„ `start_ids`ã€`end_ids` æ ‡ç­¾
- `DataLoader`ï¼š
  - è´Ÿè´£æŒ‰ batch æ‰“åŒ…æ•°æ®ã€æ‰“ä¹±é¡ºåº

å†å¾€ä¸‹æ˜¯ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼š

```python
    optimizer = torch.optim.AdamW(
        lr=args.learning_rate, params=model.parameters())

    criterion = torch.nn.functional.binary_cross_entropy
    metric = SpanEvaluator()
```

- `AdamW`ï¼šå¸¦æƒé‡è¡°å‡çš„ Adam ä¼˜åŒ–å™¨
- `criterion`ï¼šäºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆPhase 3 å·²è¯¦ç»†è®²è¿‡ï¼‰
- `SpanEvaluator()`ï¼šè®¡ç®— P/R/F1 çš„å·¥å…·ç±»

### 4.3 è®­ç»ƒä¸»å¾ªç¯ï¼šä¸€å±‚ä¸€å±‚çœ‹

è®­ç»ƒæ ¸å¿ƒåœ¨ä¸¤å±‚å¾ªç¯ä¸­ï¼š

```python
    epoch_iterator = range(1, args.num_epochs + 1)
    if show_bar:
        epoch_iterator = tqdm(epoch_iterator, desc='Training', unit='epoch')
    for epoch in epoch_iterator:
        train_data_iterator = train_data_loader
        if show_bar:
            train_data_iterator = tqdm(train_data_iterator,
                                       desc=f'Training Epoch {epoch}',
                                       unit='batch')
        for batch in train_data_iterator:
            ...
```

å«ä¹‰ï¼š
- å¤–å±‚ `for epoch in ...`ï¼šæ§åˆ¶è½®æ•°
- å†…å±‚ `for batch in train_data_iterator`ï¼šæ¯ä¸ª batch åšä¸€æ¬¡â€œå‰å‘ âœ è®¡ç®— loss âœ åå‘ âœ æ›´æ–°å‚æ•°â€

é‡ç‚¹çœ‹ `for batch in train_data_iterator:` é‡Œé¢ï¼š

```python
            input_ids, token_type_ids, att_mask, start_ids, end_ids = batch
            if args.device == 'gpu':
                input_ids = input_ids.cuda()
                token_type_ids = token_type_ids.cuda()
                att_mask = att_mask.cuda()
                start_ids = start_ids.cuda()
                end_ids = end_ids.cuda()

            outputs = model(input_ids=input_ids,
                            token_type_ids=token_type_ids,
                            attention_mask=att_mask)
            start_prob, end_prob = outputs[0], outputs[1]
```

è§£é‡Šï¼š
- `batch` æ¥è‡ª `IEDataset.__getitem__`ï¼Œå·²ç»æ‰“åŒ…å¥½ 5 ä¸ªå¼ é‡
- å¦‚æœç”¨ GPUï¼Œå°†æ‰€æœ‰å¼ é‡ `.cuda()` åˆ°æ˜¾å­˜
- `model(...)`ï¼š
  - å†…éƒ¨æ‰§è¡Œï¼šç¼–ç å™¨ + æŒ‡é’ˆç½‘ç»œï¼ˆè§ Phase 3ï¼‰
  - è¾“å‡ºä¸¤ç»„æ¦‚ç‡å›¾ï¼š`start_prob`ã€`end_prob`

æ¥ç€æ˜¯æŸå¤±å’Œåå‘ï¼š

```python
            start_ids = start_ids.type(torch.float32)
            end_ids = end_ids.type(torch.float32)
            loss_start = criterion(start_prob, start_ids)
            loss_end = criterion(end_prob, end_ids)
            loss = (loss_start + loss_end) / 2.0
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
```

å«ä¹‰ï¼š
- æ ‡ç­¾åŸæœ¬æ˜¯ 0/1ï¼ˆæ•´å‹ï¼‰ï¼Œè¿™é‡Œè½¬ä¸º `float32`ï¼Œä»¥é€‚é… `binary_cross_entropy`
- `loss_start / loss_end`ï¼šèµ·æ­¢ä¸¤ä¸ªæŒ‡é’ˆçš„ BCE Loss
- `loss = (loss_start + loss_end) / 2.0`ï¼šå¹³å‡ä¸€ä¸‹ï¼Œä¿æŒå¹³è¡¡
- `loss.backward()`ï¼šè®¡ç®—æ¢¯åº¦
- `optimizer.step()`ï¼šæ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°
- `optimizer.zero_grad()`ï¼šæ¸…ç©ºä¸Šä¸€æ¬¡çš„æ¢¯åº¦

### 4.4 è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—ä¸è¯„ä¼°

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‘¨æœŸæ€§è¯„ä¼°ï¼š

```python
            global_step += 1
            if global_step % args.logging_steps == 0:
                ...
            if global_step % args.valid_steps == 0:
                dev_loss_avg, precision, recall, f1 = evaluate(
                    model, metric, data_loader=dev_data_loader,
                    device=args.device, loss_fn=criterion)
                ...
                if f1 > best_f1:
                    save_dir = os.path.join(args.save_dir, "model_best")
                    model_to_save = model
                    model_to_save.save_pretrained(save_dir)
                    tokenizer.save_pretrained(save_dir)
```

å…³é”®ç‚¹ï¼š
- `logging_steps`ï¼šæ§åˆ¶æ‰“å°è®­ç»ƒ loss çš„é¢‘ç‡
- `valid_steps`ï¼šæ§åˆ¶åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°çš„é¢‘ç‡
- `evaluate(...)`ï¼šè®¡ç®— dev_lossã€P/R/F1
- `if f1 > best_f1:`ï¼šåªåœ¨ F1 æå‡æ—¶æ›´æ–° `model_best/`

å¦‚æœä½ å¼€å¯äº† `--early_stopping`ï¼Œæœ€åä¸€æ®µæ˜¯æ—©åœé€»è¾‘ï¼ˆå½“éªŒè¯é›† loss è¿ç»­å¤šæ¬¡ä¸ä¸‹é™å°±åœæ­¢è®­ç»ƒï¼‰ã€‚

---

## 5. ç¬¬ä¸‰æ­¥ï¼šç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åšæ¨ç†

è¿™ä¸€æ­¥çš„ç›®æ ‡ï¼šæ‹¿ `checkpoint_debug/model_best` é‡Œçš„æƒé‡ï¼Œè·‘ä¸€æ¬¡å®é™…é¢„æµ‹ã€‚

### 5.1 å¿«é€Ÿä½“éªŒï¼šç›´æ¥è·‘ `uie_predictor.py`

æ–‡ä»¶ï¼š`uie_pytorch/uie_predictor.py`

åº•éƒ¨ç¤ºä¾‹ï¼š

```python
if __name__ == '__main__':
    args = parse_args()
    args.schema = ['èˆªæ¯']
    args.schema_lang = "en"
    uie = UIEPredictor(
        model=args.model,
        task_path=args.task_path,
        schema_lang=args.schema_lang,
        schema=args.schema,
        engine=args.engine,
        device=args.device,
        position_prob=args.position_prob,
        max_seq_len=args.max_seq_len,
        batch_size=64,
        split_sentence=False,
        use_fp16=args.use_fp16)
    print(uie("å°åª’æ‰€ç§°çš„â€œå°åº¦ç¬¬ä¸€è‰˜å›½äº§èˆªæ¯â€â€”â€œç»´å…‹å…°ç‰¹â€å·"))
```

å¦‚æœä½ ç›´æ¥è·‘ï¼š

```bash
cd uie_pytorch
python3 uie_predictor.py -m bert-base-chinese -D cpu
```

ä¼šåŠ è½½ `bert-base-chinese` å¯¹åº”çš„ UIE æ¨¡å‹ï¼Œå¹¶æŒ‰ç…§ `schema=['èˆªæ¯']` æŠ½å–æ–‡æœ¬ä¸­çš„â€œèˆªæ¯â€å®ä½“ã€‚è¿™åªæ˜¯å®˜æ–¹ Demoã€‚

### 5.2 ä½¿ç”¨ä½ è®­ç»ƒå¥½çš„æ¨¡å‹åšæ¨ç†

æˆ‘ä»¬å¸Œæœ›ç”¨è‡ªå·±çš„ checkpointï¼Œå› æ­¤æ¨èå†™ä¸€ä¸ªç®€å•çš„æ¨ç†è„šæœ¬ï¼ˆç¤ºä¾‹ï¼‰ï¼š

```python
# æ–‡ä»¶ï¼špredict_debug.pyï¼ˆä½ å¯ä»¥æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰
from uie_pytorch.uie_predictor import UIEPredictor

if __name__ == "__main__":
    schema = ["ç–¾ç—…", "è¯ç‰©"]  # æƒ³æŠ½å–çš„å­—æ®µ

    uie = UIEPredictor(
        model="bert-base-chinese",                 # ä¸è®­ç»ƒæ—¶ä¸€è‡´
        task_path="checkpoint_debug/model_best",   # è®­ç»ƒäº§å‡ºçš„ç›®å½•
        schema=schema,
        schema_lang="zh",
        engine="pytorch",
        device="cpu",
        position_prob=0.5,
        max_seq_len=512,
        batch_size=32,
        split_sentence=False,
        use_fp16=False,
    )

    text = "æ‚£è€…ç¡®è¯Šä¸ºç³–å°¿ç—…ï¼Œç»™äºˆäºŒç”²åŒèƒæ²»ç–—ã€‚"
    print(uie(text))
```

è¿è¡Œï¼š

```bash
python3 predict_debug.py
```

ä½ åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼ï¼š

```python
[{
  "ç–¾ç—…": [{"text": "ç³–å°¿ç—…", "start": 4, "end": 7, "probability": 0.97}],
  "è¯ç‰©": [{"text": "äºŒç”²åŒèƒ", "start": 10, "end": 13, "probability": 0.95}]
}]
```

### 5.3 è¯»æ‡‚ `UIEPredictor` çš„æ ¸å¿ƒæµç¨‹

ç±»å®šä¹‰åœ¨ `uie_pytorch/uie_predictor.py` ä¸­ï¼š

#### (1) åˆå§‹åŒ–ï¼šåŠ è½½æ¨¡å‹ + Tokenizer

```python
class UIEPredictor(object):

    def __init__(self, model, schema, task_path=None, schema_lang="zh",
                 engine='pytorch', device='cpu', position_prob=0.5,
                 max_seq_len=512, batch_size=64, split_sentence=False,
                 use_fp16=False, multilingual=False):
        ...
        self._schema_tree = None
        self._is_en = True if model in ['uie-base-en'] or schema_lang == 'en' else False
        self.set_schema(schema)
        self._prepare_predictor()
```

ç†è§£ï¼š
- `schema`ï¼šä½ è¦æŠ½å–çš„ä»»åŠ¡æè¿°ï¼Œå¯ä»¥æ˜¯ç®€å•åˆ—è¡¨æˆ–åµŒå¥—å­—å…¸
- `schema_lang`ï¼šPrompt ç”¨ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡
- `set_schema(...)`ï¼šæŠŠ Python åˆ—è¡¨/å­—å…¸å˜æˆä¸€æ£µ `SchemaTree`
- `_prepare_predictor()`ï¼š
  - æ ¹æ® `engine` é€‰æ‹© PyTorch æˆ– ONNX
  - åŠ è½½ Tokenizer å’Œæ¨¡å‹æƒé‡

#### (2) è°ƒç”¨å…¥å£ï¼š`__call__` / `predict`

```python
    def __call__(self, inputs):
        texts = inputs
        if isinstance(texts, str):
            texts = [texts]
        results = self._multi_stage_predict(texts)
        return results

    def predict(self, input_data):
        results = self._multi_stage_predict(input_data)
        return results
```

å³ï¼š
- ä½ åœ¨ Python é‡Œç›´æ¥è°ƒç”¨ `uie("ä¸€æ®µæ–‡æœ¬")` æˆ– `uie.predict([...])`
- å†…éƒ¨ç»Ÿä¸€èµ° `_multi_stage_predict`

#### (3) å¤šé˜¶æ®µé¢„æµ‹ `_multi_stage_predict`

```python
    def _multi_stage_predict(self, datas):
        results = [{} for _ in range(len(datas))]
        if len(datas) < 1 or self._schema_tree is None:
            return results

        schema_list = self._schema_tree.children[:]
        while len(schema_list) > 0:
            node = schema_list.pop(0)
            examples = []
            input_map = {}
            ...
            if not node.prefix:
                # ç¬¬ä¸€å±‚ï¼šç›´æ¥ç”¨ schema åš prompt
                for data in datas:
                    examples.append({
                        "text": data,
                        "prompt": dbc2sbc(node.name)
                    })
                    ...
            else:
                # å­èŠ‚ç‚¹ï¼šç”¨â€œçˆ¶èŠ‚ç‚¹æŠ½åˆ°çš„å®ä½“â€æ‹¼æ¥ prompt
                ...

            if len(examples) == 0:
                result_list = []
            else:
                result_list = self._single_stage_predict(examples)
            ...
```

å¯ä»¥è¿™æ ·ç†è§£ï¼š
- ç¬¬ä¸€è½®ï¼šç”¨æœ€ä¸Šå±‚ schemaï¼ˆæ¯”å¦‚ `"ç–¾ç—…"`ã€`"è¯ç‰©"`ï¼‰ä½œä¸º promptï¼Œè·‘ä¸€éæ¨¡å‹
- åç»­è½®æ¬¡ï¼šå¦‚æœ schema æ˜¯åµŒå¥—çš„ï¼ˆä¾‹å¦‚ `{"ç–¾ç—…": ["å¹¶å‘ç—‡", "æ²»ç–—æ–¹æ¡ˆ"]}`ï¼‰ï¼Œå°±ä¼šæŠŠä¸Šä¸€è½®æŠ½å–å‡ºæ¥çš„å®ä½“æ–‡æœ¬ï¼Œæ‹¼æˆæ›´å…·ä½“çš„ promptï¼Œå†è·‘ä¸€éæ¨¡å‹
- æœ€ç»ˆé€šè¿‡ `results` æŠŠæ‰€æœ‰å±‚çº§çš„æŠ½å–ç»“æœæ‹¼æˆä¸€ä¸ªç»“æ„åŒ–å­—å…¸è¿”å›

#### (4) é•¿æ–‡æœ¬åˆ‡åˆ† `_auto_splitter` & ç»“æœåˆå¹¶ `_auto_joiner`

å½“è¾“å…¥æ–‡æœ¬å¤ªé•¿æ—¶ï¼š
- `_auto_splitter`ï¼šæ ¹æ® `max_seq_len` è‡ªåŠ¨åˆ‡æˆå¤šæ®µ
- `_auto_joiner`ï¼šæŠŠæ¯æ®µç»“æœçš„ `start/end` ä½ç½®é‡æ–°å¯¹é½åˆ°åŸå§‹æ–‡æœ¬

è¿™éƒ¨åˆ†åœ¨é•¿æŠ¥å‘Šã€é•¿ç—…å†åœºæ™¯ä¸‹éå¸¸é‡è¦ã€‚

---

## 6. å®æˆ˜ä»»åŠ¡æ¸…å•ï¼ˆå»ºè®®ä½ çœŸæ­£åšä¸€éï¼‰

1. è·‘ `convert_data.py`ï¼Œæ‰“å¼€ `train_converted.jsonl`ï¼Œå¯¹ç…§ `cmeie_to_uie_samples` æ‰‹å·¥æ‰¾ 1 æ¡æ ·æœ¬ï¼Œçœ‹æ˜¯å¦ç†è§£ï¼š
   - åŸå§‹ `spo_list`
   - ç”Ÿæˆäº†å‡ æ¡ UIE æ ·æœ¬
2. æ‰“å¼€ `run_training_debug.sh`ï¼Œä¿®æ”¹ï¼š
   - `--num_epochs` æ”¹æˆ 2
   - `--learning_rate` æ”¹æˆ `5e-5`
   - é‡æ–°è¿è¡Œè„šæœ¬ï¼Œè§‚å¯Ÿ loss å’Œ F1 çš„å˜åŒ–
3. åœ¨ `finetune.py` ä¸­ï¼š
   - æ‰¾åˆ° `optimizer = AdamW(...)`ï¼Œæ€è€ƒå¦‚æœæ”¹æˆ `Adam` ä¼šæœ‰ä»€ä¹ˆå½±å“ï¼ˆå¯æœ¬åœ°å®éªŒï¼‰
4. å†™ä¸€ä¸ªè‡ªå·±çš„ `predict_debug.py`ï¼š
   - è‡ªå®šä¹‰ `schema`ï¼Œä¾‹å¦‚ `["ç–¾ç—…", "æ‰‹æœ¯", "æ£€æŸ¥"]`
   - è¾“å…¥å‡ æ¡çœŸå®çš„ç—…å†æ–‡æœ¬ï¼Œè§‚å¯ŸæŠ½å–æ•ˆæœ
5. é˜…è¯» `uie_predictor.py` ä¸­ `_convert_ids_to_results`ï¼Œç¡®è®¤ä½ èƒ½è¯´å‡ºï¼š
   - `start/end` ç´¢å¼•æ˜¯å¦‚ä½•æ˜ å°„å›åŸæ–‡çš„
   - `probability` æ˜¯æ€ä¹ˆä»æ¨¡å‹è¾“å‡ºçš„ logits è½¬æ¢æ¥çš„

---

## 7. å°ç»“ï¼šæŠŠå‰å‡ ç« å†…å®¹çœŸæ­£â€œè·‘è¿›ä»£ç é‡Œâ€

é€šè¿‡æœ¬ç« ä½ åº”è¯¥åšåˆ°ï¼š

- èƒ½ä»é¡¹ç›®æ ¹ç›®å½•ä¸€æ¡å‘½ä»¤ä¸€æ¡å‘½ä»¤åœ°è·‘é€šå®Œæ•´è®­ç»ƒæµç¨‹
- çŸ¥é“ `convert_data.py`ã€`run_training_debug.sh`ã€`finetune.py`ã€`uie_predictor.py` å„è‡ªçš„è´£ä»»
- å¯¹ `do_train` å’Œ `UIEPredictor` çš„æ¯ä¸€æ®µæ ¸å¿ƒä»£ç ï¼Œéƒ½èƒ½ç”¨è‡ªå·±çš„è¯è§£é‡Šâ€œä¸ºä»€ä¹ˆè¦è¿™æ ·å†™â€

å»ºè®®åœ¨ Phase 4 å®Œæˆåï¼Œå†å›å¤´çœ‹ Phase 2 / Phase 3 çš„ç†è®ºéƒ¨åˆ†ï¼Œä¼šæ›´æœ‰æ„Ÿè§‰â€”â€”å› ä¸ºä½ å·²ç»çœŸæ­£â€œç”¨è¿™äº›ä»£ç æŠŠæ¨¡å‹è·‘èµ·æ¥â€äº†ã€‚
